\chapter{Statistical Process Control- SPC}
\label{sec:spc}

Statistical process control, \aka \emph{change detection algorithms} deals with the quantitative analysis of a ``process'', which may be a production line, a service, or any other repeated operation.\marginnote{Change Detection}
As such, it may be found in the Analyze, Improve, and Control stages of the DMAIC cycle.
The purpose of the SPC, in the terms coined by Shewart, is to seperate the variability in the process into \emph{assignable} causes of variation and \emph{chance} causes of variation.\marginnote{Causes of variation}
These are also known as \emph{special} and \emph{common} causes of variation, respectively. 
A process is said to be in \emph{statistical control} if all its variation is attributable to chance causes.
If this is not the case, we call it \emph{out of control} and we will seek the assignable causes, remove then, and   re-analyze.


All the previously mentioned statistical tools may be called upon for this analysis. 
In the context of process control, a subset of tools has gained the nick-name ``The Magnificent Seven''. These include:
\begin{description}
\item Histogram and stem-and-leaf plot. As described in Chapter~\ref{sec:exploratory}.
\item Check Sheet. [TODO: add figure]
\item Pareto chart. An ordered bar plot of the events due to the various assignable variability causes. [TODO: add figure]
\item Cause-and-effect diagram. A visualization of candidate assignable variability causes. [TODO: add figure]
\item Defect concentration diagram. A visual inspection of the location of defects on the product. 
\item Scatter plot. As described in Chapter~\ref{sec:exploratory}.
\item Control chart. A powerful analysis tool to which we devote the rest of this chapter. 
\end{description}





%\begin{pgfpicture}
%    \pgftext{\pgfimage[width=0.6\linewidth, height=0.3\textheight]{}}
%\end{pgfpicture}



\begin{extra}[A more rigorous treatment ]
The contents of this chapter is mostly derived from \cite{montgomery_introduction_2007}. 
For a more mathematically rigorous treatment of the topic see \cite{basseville_detection_1993}.
For an \R oriented exposition of the topic, see \cite{qiu_introduction_2013}.
\end{extra}

\section{A soft start. The \barxChart}


% basic idea
% descion variables: sample size, intervals, statistic, limits, rational groupins, type I error rate, multiple criteria
% phase I and II.
% what to do in case of alarm?
% extensions: probability limits, other statisics=, sample , adaptive parameters, other rules, non normality, variable sample size, moving windows
% considerations for setting these values.
% Setting limits: history, bootstrap, CLT
% multiplicity in control charts
% FDR controlling limits
% western electric rules. The type I error probability of the rule.
% nelson rules


We demonstrate the concepts and utility of Control Charts with the simplest, yet most popular of them all, the \barxChart. 
The chart borrows its name from the fact that it is essentially a visualization of the time evolution of the average ($\bar{x}$) of the CTQ of a sample of products. 
The chart is also augmented with visual aids that help in determining if the process is \emph{in control}, i.e., if it consistent with its own history. 
Process capability analysis may benefit from the ideas of control charts. We emphasize however, that control charts have no information on the specifications of the process, merely on its own history.

An illustration of a \barxChart is given in Figure~\ref{fig:bar_x_chart}. 
The ingredients of this chart is the centerline, the control limits, and $\bar{x}$ evolving in time. 
If at each period $t=1,\dots,\tau$, we compute the average of $n$ samples, we denote $\bar{x}_t:=1/n \sum_{i=1}^n x_{it}$.

\begin{figure}[h]
\centering
\includegraphics[height=0.3\textheight]{art/X-chartExample}
\caption[\barxChart]{\barxChart. \newline \url{https://mvpprograms.com/help/P-mvpstats/spc/WhatAreControlCharts}}
\label{fig:bar_x_chart}
\end{figure}






Figure~\ref{fig:bar_x_chart} makes it evident \barxChart requires several design decisions.
A standard design decision is setting the center line as the grand average of the process: 
\begin{align}
\label{eq:centerline}
	1/\tau \sum_{t=1}^\tau \bar{x}_t
\end{align}


If it is unclear to you, how may we compute the grand average of a process that is still evolving and has not finished, you are right! We thus introduce the ideas of \emph{Phase I} and \emph{Phase II}. \marginnote{Phase I/II}
Initially we assume the process it out of control, we identify and remove assignable causes of variation, until we are left with a ``well-behaved'' subset of data points. We call this Phase I, and we use it to initialize required quantities such as the centre line. 
Eq.(\ref{eq:centerline}) thus implies that in Phase I we were left with $\tau$ samples assumingly in statistical control.
After the chart has been calibrated, and major assignable sources of variability removed, we can finally start monitoring the process, known as Phase II.



Other design decisions to be made are:
\begin{enumerate}
\item UCL and LCL \footnote{Do not confuse with USL and LSL!}.
\item Sample size in each sample.
\item Rational groupings (within-period sampling scheme).
\item Frequency of samples (between-period sampling scheme). 
\item Other stopping rules.
\end{enumerate}
These deign decisions ultimately govern the error rate (false positive rate) and power (false negative rate) of the chart, which in turn, incur some financial costs. 
For now we will restrict attention to type I/II error rates, until Section~\ref{sec:economical_considerations} where we consider these choices as economical optimization problems.

For ease of exposition, control chart design is demonstrated for the \barxChart, but equally apply to other control charts.
We start by a type I error rate analysis. 
Denote $\alpha_t$ the false alarm probability at period $t$.
How do our design choices affect $\alpha_t$?
\begin{align}
	\alpha_t &:= 1-P_{H_0}(\bar{x}_t \in [UCL,LCL]) \\
	&= 2 P_{H_0}(\bar{x}_t<UCL) \\
	&= 2 P_{H_0}(Z<\frac{UCL-\mu}{\sigma_{\bar{x}}}) \\
	&= 2 P_{H_0}(Z < -k) \\
	&= 2 \Phi(-k)
\end{align}
The above follows from assuming that $UCL:=\mu + k \sigmabar, LCL:= \mu - k \sigmabar$, $\x_{it}\sim \gauss{\mu,\sigma^2}$, and denoting $\sigmabar:= \frac{\sigma}{n}$.
A typical design choice is $k=3$, known as \emph{3-sigma control limits}, implying a false alarm rate of $\alpha_t=0.0027$.\marginnote{3-Sigma Control Limits}
Since we assumed the process is fixed over time, then so is $\alpha_t$ and we can simply write $\alpha_t=\alpha$.

A power analysis for our design choices follows the same lines.
Denote $\beta_t$, and $\pi_t=1-\beta_t$ the type II error rate, and power, at period $t$.
We then have
\begin{align}
	\pi_t &:= 1-P_{H_1}(\bar{x}_t \in [UCL,LCL])
\end{align}
and the rest follow from the distribution of $\bar{x}$ when the process is out of control.
Assuming the out-of-control process is a shift of magnitude $k \sigma$, i.e.: $\x \sim \gauss{\mu_1,\sigma^2}; \mu_1=k \sigma$, we plot in Figure~\ref{fig:power_function}, the detection power of a 3-sigma chart, as a function of $k$. 
This is known as statistical literature as a \emph{power function}, and in the engineering literature as the \emph{true positive rate} operator characteristic (TPR-OR.)


\begin{figure}[h]
\centering
\includegraphics[height=0.3\textheight]{art/power_function.pdf}
\caption[Power Function]{Power function of the 3-sigma \barxChart with $n=5,10,20$ and $\mu_1=k \sigma$.}
\label{fig:power_function}
\end{figure}


\paragraph{ARL}
Another important related quantity is the \emph{average run length} (ARL), which is the expected number of periods between two crossings of control limits. \marginnote{ARL}
We denote by $ARL_0$ the average run length when the process is under statistical control, and $ARL_1$ otherwise\footnote{Note that it is implied that the process has a \emph{stable} distribution, even though it is out if control.}. 
If $\bar{x}_t$ are statistically independent, then clearly the number of periods until a crossing is geometrically distributed. Using the expectation of a geometric random variable we can conclude that 
\begin{align}
	ARL_0=1/\alpha \\
	ARL_1=1/\pi
\end{align}
Clearly we can convert to time units by multiplying the ARL by the duration of sampling interval.
This is known as the \emph{average time to signal} (ATS).\marginnote{ATS}


Now assume that we are unhappy with some control chart. 
It simply makes too many false alarms, or takes too long to detect loss of statistical control.
What can we do about it?
Well, this is exactly the same question as when increasing the power or lowering the type I error of a statistical hypothesis test. This is obviously no coincidence, since control charts are nothing but a statistical test!
Here are some action courses:
\begin{enumerate}
\item Increase $k$. This is the same as shrinking a rejection region: 
it will decrease the false alarm rate, at the cost of statistical sensitivity.
\item Increase $n$. Brilliant! Statistically, there is nothing to lose. It may, however, cost time and money.
\item Increase the sampling frequency. Brilliant again! Nothing to lose, except time and money...
\item Change the sampling scheme within period. We elaborate on this in Section~\ref{sec:rational_grouping}.
\item Add other stopping rules: 
this acts just like growing a rejection region. It will increase power, at the cost of type I error. We elaborate in Section~\ref{sec:stopping_rules}.
\item Pool together more time points or more variables. We elaborate on this in sections \ref{sec:running_windows} and  \ref{sec:multivariate}, respectively. 
\end{enumerate}






\subsection{Control Limits and the Alarm Rate}
As previously discussed, $k$ governs the tradeoff between type I and type II errors, or sensitivity versus specificity.
It is very common to set $k=3$. 
For a Normally distributed CTQ, this implies $2,700$ false alarms per million periods. 
This also implies an ARL of $1/\alpha=370$ periods.
We may however, discard this convention, and directly set UCL and LCL so they guarantee some desirable false alarm rate. 

If normality of $\bar{x}$ can be assumed, then one may estimate $\sigma$ from phase I, and set LCL and UCL by finding the $k$ that solves $2\Phi(-k)=\alpha$.
If normality cannot be assumed, there are many ways to go about. Here are some options:
\begin{enumerate}
\item Increase $n$: even if $\x_i$ is non normal, for large enough $n$, then $\bar{x}$ will be via the central limit theorem.
\item Use empirical quantiles: If phase I was returned enough data, then we may estimate $\x_{\alpha/2}$ and $\x_{1-\alpha/2}$ using the empirical quantiles of phase I. The false alarm rate will be $\alpha$ since $P(\x \not \in [\x_{\alpha/2},\x_{1-\alpha/2}])=\alpha$, if $[\x_{\alpha/2},\x_{1-\alpha/2}]$ is well estimated.
\item TODO:simulation
\end{enumerate}




\subsection{Rational Groupings}
\label{sec:rational_grouping}
Recall that at each period we compute the average of $n$ samples. 
How should we draw this samples? All at the same time from the same machine?
At different times from the same machine?
Many configurations are possible, and the correct approach depends on the type of out-of-control behaviour one seeks. 
\emph{Rational groupings} merely reminds us to sample ``rationally'' in each period. 
Quoting \cite{montgomery_introduction_2007}'s words of caution:
\begin{quotation}
\dots we can often make any process appear to be in statistical control just by stretching out the interval between observations in the sample.
\end{quotation}






\subsection{Other Stopping Rules}
\label{sec:stopping_rules}

The assumption that we may only create alarms if $\bar{x}$ exceeds some control limits is needle sly restrictive.
A first relaxation is by allowing multiple regions.
It is quite common to define \emph{warning limits} which only call for inspection, and \emph{action limits}. Each may have its own alarm rate.
We may even change the sapling scheme if limits are breached. Increasing the sampling rate once the warning limits have been breached is known as \emph{adaptive sampling}, or \emph{variable sampling}.\marginnote{Adaptive Sampling}



Another, more strict approach, is to define multiple sets of stopping rules
Here an example:
\begin{enumerate}
\item One or more points outside of the control limits.
\item Two of three consecutive points outside the 2-sigma warning limits but still inside the control limits.
\item Four of five consecutive points beyond the 1-sigma limits.
\item A run of 8 consecutive points on one side of the center line.
\end{enumerate}
The above set of rules is known as the Western Electric Rules, \aka\, the \emph{WECO} rules.\marginnote{WECO}
Augmenting the set of rules is the same as increasing a rejection region. It adds more sensitivity, at the cost of false alarms. If the rules are properly selected, the gain in sensitivity is worth the increase in false alarms.

As a quick exercise, we may compute $\alpha$  for $m$ independent rules, each with $\alpha^*$ type I error itself:
\begin{align}
	\alpha=1-(1-\alpha^*)^m.
\end{align}
Having 4 rules, like WECO, each at $\alpha^*0.0027$ implies that we actually have $\alpha=0.01$.

\begin{extra}[Stopping Rules]
There are many sets of stopping rules. 
These include WECO, Nelson, AIAG, Juran, Hughes, Duncan, Gitlow, Westgard, and more. 
See \url{http://www.quinn-curtis.com/spcnamedrulesets.htm} for a quick introduction.
\end{extra}



\section{Running Window Charts}
\label{sec:running_windows}

Assume an out-of-control process is simply a mild shift of the controlled-process.
This shift may be hard to detect in Shewart chart, especially if $n$ is not too large (as seen in Figure~\ref{fig:power_function}).  
If the shift persists, we may gain power, i.e., sensitivity, by pooling several periods together. 
This can be done in several ways.



\subsection{$EWMA$ Chart}


\subsection{Moving Average Chart}


\subsection{Filtered Derivative Chart}


\subsection{$cusum$ Chart}
The fundamental idea in a cumulative sum chart, or \emph{cusum} chart, is to sum deviation from the centre line.
If the process is in control, deviation will cancel each other, and their sum will vary around $0$. 
If the process is ot of control, some drift will appear in the cusum. 
The statistic to be plotted is 
\begin{align}
	C_t:= \sum_{j=0}^{t}(\bar{x}_j-\mu_0)=C_{t-1}+ (\bar{x}_t-\mu_0)
\end{align} 



\begin{extra}[Wald's Sequential Likelihood Ration Test- SPRT]
[TODO]
\end{extra}




\subsection{Combined Shewart and Running Window Charts}






\begin{extra}[Local Methods]
% scan statistic
% sliding window
% search light
% Anomaly detection active learning
\end{extra}








\section{Multivariate Control Charts}
\label{sec:multivariate}
% Wishart
% Srivastava Du
% PCA
% Higher criticism




\section{Economical Design of Control Charts}
\label{sec:economical_considerations}






\section{Non-Statistical Target Functions}






\section{Other Control Statistics}
\subsection{$R$ Chart}
\subsection{$s$ Chart}
\subsection{$s^2$ Chart}
\subsection{Shewhart Individuals Control Chart}
\subsection{Three-way Chart}
\subsection{$p$ Chart}
\subsection{$np$ Chart}
\subsection{$c$ Chart}
\subsection{$u$ Chart}
\subsection{Time Series Model}
\subsection{Regression Control Chart}
\subsection{Running Window Versions}




\section{Notes}
% White noise process
% One sided control chart


