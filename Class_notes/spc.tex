\chapter[Statistical Process Control]{Statistical Process Control}
\chaptermark{SPC}
\label{sec:spc}

Statistical process control (SPC), \aka \emph{change detection}, or \emph{novelty detection}, deals with the quantitative analysis of a ``process'', which may be a production line, a service, or any other repeated operation.\marginnote{Change Detection}
As such, SPC may be found in the Analyze, Improve, and Control stages of the DMAIC cycle.
The purpose of the SPC, in the terms coined by Shewhart, is to seperate the variability in the process into \emph{assignable} causes of variation and \emph{chance} causes of variation.\marginnote{Causes of variation}
Assignable are also known as \emph{special} causes, or simply \emph{signal}.
Chance causes are also known as \emph{common} causes of variation, or \emph{haphazard} variability, or simply \emph{noise}.
 
A process is said to be in \emph{statistical control} if all its variation is attributable to chance causes.
If this is not the case, we call it \emph{out of control} and we will seek the assignable causes, so that we may reduce variability by removing them.
All the statistical tools of chapters \ref{sec:exploratory} and \ref{sec:inference} may be called upon for this endeavour but in this chapter we focus on one particular such tool- the \emph{control chart}.
We start with the \emph{Shewhart control chart}, in which each value is charted using different data, from different periods. \marginnote{Shewhart Chart}



\section{The Run Chart}
The simplest possible control chart is the \emph{run chart}, in which each measured CTQ is simply plotted against the time of measurement.
If strong anomalies exist, or temporal patterns, they may be already visible in the run-chart.
On the down-side, the run-chart is essentially a statistical test to detect out-of-control behaviour based on a single observation at a time.
Knowledge of statistical hypothesis testing suggest we can do better. 
Enter the \barxChart. 


\section[The \barxChart]{The \barxChart}
\sectionmark{\barxChart}


We demonstrate the concepts and utility of control charts with the simplest, yet most popular of them all, the \barxChart. 
The chart borrows its name from the fact that it is essentially a visualization of the time evolution of the average ($\bar{x}$) of the CTQ. 
The chart is also augmented with visual aids that help in determining if the process is \emph{in control}, i.e., if it is consistent with its own history. 

\begin{remark}[Control Charts and Capability Analysis]
While seemingly very similar ideas, there is a fundamental difference between capability analysis and process control:  process control compares to the \emph{past}, capability analysis compares to \emph{specification}.
Process capability and control charting ideas may be compounded, as we explain in Chapter~\ref{sec:advanced_capability_analysis}.
\end{remark}


An illustration of a \barxChart is given in Figure~\ref{fig:bar_x_chart}. 
The ingredients of this chart are the \emph{centerline}, lower and upper control limits (\emph{LCL, UCL}), and the statistic $\bar{x}_t$ evolving in time. 
If at each period $t=1,\dots,\tau$ we compute the average of the $n$ samples of the period. We denote $$\bar{x}_t:=1/n \sum_{i=1}^n x_{it}.$$

\begin{figure}[ht]
\centering
\includegraphics[height=0.3\textheight]{art/X-chartExample}
\caption[\barxChart]{\barxChart. \newline \url{https://mvpprograms.com/help/P-mvpstats/spc/WhatAreControlCharts}}
\label{fig:bar_x_chart}
\end{figure}



Figure~\ref{fig:bar_x_chart} makes it evident \barxChart requires us to make several design decisions.
A standard design decision is setting the centerline as the grand average of the process: 
\begin{align}
\label{eq:centerline}
	\hat{\mu}_0=1/\tau \sum_{t=1}^\tau \bar{x}_t,
\end{align}
where $\mu_0$ denotes the in-control mean of the process. 
Notation originates from treating the in-control process as a null hypothesis, as it should be thought of.


\paragraph{Phase I/II} If it is unclear to you, how may we compute the grand average of a process that is still evolving and has not finished, you are right! We thus introduce the idea of \emph{Phase I} and \emph{Phase II}. 
Initially we assume the process it out of control, we identify and remove assignable causes of variation, until we are left with a ``well-behaved'' subset of data points, we believe to be in-control. We call this Phase I, and we use it to initialize required quantities such as the centre line $(\hat{\mu}_0)$. 
Eq.(\ref{eq:centerline}) thus implies that in Phase I we were left with $\tau$ samples assumingly in statistical control.
After the chart has been calibrated, and major assignable sources of variability removed, we can finally start monitoring the process, known as Phase II.

\bigskip

Back to the design decisions we make when designing a control chart.
\begin{tcolorbox}[breakable]
\paragraph{Design decisions}
\begin{enumerate}
\item Centerline.
\item Upper and lower confidence limits: UCL and LCL (do not confuse with USL and LSL!).
\item Sample size in each sample, denoted $n$.
\item The within period sampling scheme, known as \emph{rational groupings}.
\item The between-period sampling scheme, notably the \emph{frequency of samples}, denoted $h$. 
\item Other stopping rules.
\end{enumerate}
\end{tcolorbox}

These design decisions ultimately govern the error rates of the chart, which in turn, incur financial costs. 
For now we will restrict attention to type I/II error rates, until Section~\ref{sec:economical_considerations} where we consider these choices as an economical optimization problem.

For ease of exposition, control chart design is demonstrated for the \barxChart, but equally applies to other control charts, presented in Section~\ref{sec:other_control_charts}.
We start by a type I error rate analysis. 
Denote $\alpha_t$ the false alarm probability at period $t$.
How do our design choices affect $\alpha_t$?
\begin{align}
	\alpha_t &:= 1-P_{H_0}(\bar{x}_t \in [LCL,UCL]) \\
	&= 2 P_{H_0}(\bar{x}_t < LCL) \\
	&= 2 P_{H_0}(Z<\frac{LCL-\mu}{\sigma_{\bar{x}}}) \\
	&= 2 P_{H_0}(Z < -\arm) \\
	&= 2 \Phi(-L)
\end{align}
The above follows from assuming that $UCL:=\mu_0 + \arm \sigmabar, LCL:= \mu_0 - \arm \sigmabar$, $\x_{it}\sim \gauss{\mu,\sigma^2}$, and $\sigmabar$ being the standard deviation of the statistic being monitored; In this case, $\sigmabar=\frac{\sigma}{\sqrt{n}}$.
A typical design choice is $\arm=3$, known as \emph{3-sigma control limits}, implying a false alarm rate of $\alpha_t=0.0027$.\marginnote{3-Sigma Control Limits}
Since we assumed the process is fixed over time, then so is $\alpha_t$ and we can simply write $\alpha_t=\alpha$.

A power analysis for our design choices follows the same lines.
Denote by $H_1$ the out-of-control distribution,  $\beta_t$ the type-II error rate, and $\pi_t=1-\beta_t$ the power, at period $t$.
We then have
\begin{align}
	\pi_t &:= 1-P_{H_1}(\bar{x}_t \in [LCL,UCL])
\end{align}
and the rest follow from the distribution of $\bar{x}_t$ when the process is out of control.
Since the out-of-control shift is (asumingly) stable, we can again omit the time index and write $\pi=\pi_t$.
Assuming the out-of-control process is a shift of magnitude $k \sigma$, i.e.: $\x \sim_{H_1} \gauss{\mu_1,\sigma^2}; \mu_1=\mu_0+ k \sigma$, we plot in Figure~\ref{fig:power_function}, the detection power of a 3-sigma \barxChart as a function of $k$. 
This is known in the statistical literature as a \emph{power function}, and in the engineering literature as the \emph{operator characteristic} (OC).\marginnote{Operator Characteristic}


\begin{figure}[h]
\centering
\includegraphics[height=0.3\textheight]{art/power_function.pdf}
\caption[Power Function]{Power function of the 3-sigma \barxChart with $n=5,10,20$ and $\mu_1=\mu_0 + k \sigma$.}
\label{fig:power_function}
\end{figure}

\begin{remark}[3-Sigma Control Limits vs. 3-Sigma Capability]
Do not confuse these two similar ideas.
3-Sigma Control Limits is a statement on the false alarm rate. It tells you nothing on the probability of non-compliance. 
3-Sigma Capability is a statement on the probability of a unit to be defect. 
Monitoring production via its capability is very rare, as we would like the alarms to sounds long before defect units leave the production line.
\end{remark}



\begin{extra}[Operator Characteristics]
Many operator characteristics have been proposed to study the performance of control charts, statistical tests, or binary classifiers in general.
You may be already familiar with some, such as the Reciever Operator Charachteristic (ROC). 
The curious reader is referred to \cite{wikipedia_receiver_2015} for more information.
\end{extra}


A very important quantity is the \emph{average run length} (ARL), which is the expected number of periods between two crossings of control limits, i.e., the expected periods between alarms. \marginnote{ARL}
We denote by $ARL_0$ the ARL when the process is under statistical control, and $ARL_1$ otherwise\footnote{Note that it is implied that the process has a \emph{stable} distribution, even though it is out if control.}. 
For Shewhart charts, where $\bar{x}_t$ are statistically independent and $\alpha_t,\pi_t$ fixed in time, then clearly the number of periods until a crossing is geometrically distributed. Using the expectation of a geometric random variable we can conclude that 
\begin{align}
	ARL_0=1/\alpha \label{eq:arl_0}, \\
	ARL_1=1/\pi \label{eq:arl_1}.
\end{align}
ARL is measured in periods. 
We can convert to time units by multiplying the ARL by the duration of sampling interval ($h$).
This is known as the \emph{average time to signal} (ATS).\marginnote{ATS}
It is quite common to design a control chart so that it achieves a particular $ATS_0$.

\begin{remark}[ARL more important than type-I error]
In the case of Shewhart charts, there is a simple mapping between ARL and type I error rates.
This need not be the case for general control charts. 
Since type I errors will occur with certainty if the process runs long enough, then it is actually the ARL that is more informative than type-I errors when designing a control chart.
\end{remark}


Now assume that we are unhappy with our control chart. 
It simply makes too many false alarms, or takes too long to detect loss of statistical control.
What can we do about it?
Well, this is exactly the same question as when increasing the power or lowering the type I error of a statistical hypothesis test. This is obviously no coincidence, since control charts are nothing but a statistical test!
Here are some action courses:
\begin{enumerate}
\item Increase $\arm$. This is the same as shrinking the rejection region: 
it will decrease the false alarm rate, at the cost of power.
\item Increase $n$. Brilliant! Statistically, there is nothing to lose. It may, however, cost time and money.
\item Increase the sampling frequency $h$. Brilliant again! Nothing to lose, except time and money.
\item Change the sampling scheme within period. We elaborate on this in Section~\ref{sec:rational_grouping}.
\item Add other stopping rules: 
this acts just like growing the rejection region. It will increase power, at the cost of type I error. 
We elaborate in Section~\ref{sec:stopping_rules}.
\item Pool together more periods. See Section~\ref{sec:running_windows}.
\item Measure many CTQs simultaneously. See Section~\ref{sec:multivariate}. 
\end{enumerate}






\subsection{Control Limits and the Alarm Rate}
Ceteris paribus, $\arm$ governs the tradeoff between type I and type II errors, or sensitivity versus specificity.
It is very common to set $\arm=3$. 
For a normally distributed CTQ, this implies $2,700$ false alarms per million periods. 
This also implies an $ARL_0$ of $1/\alpha \approx 370$ periods, which is conveniently, about a year if sampling once a day.
We may obviously, discard this $\arm=3$ convention, and directly set UCL and LCL so they guarantee some desirable false alarm rate, or ARL.

If normality of $\bar{x}_t$ can be assumed, then one may estimate $\sigma$ from phase I, and set LCL and UCL by finding the $\arm$ that solves $2\Phi(-\arm)=\alpha$, for some desired $\alpha$. 
If normality cannot be assumed, there are many ways to go about. Here are some options:
\begin{enumerate}
\item If some other distribution can be assumed then problem solved.
We may compute the false alarm rate of particular limits either analytically, or computationally (by simulation).
\item Increase $n$: even if $\x_{it}$ is non normal, for large enough $n$, then $\bar{x}_t$ will be via the central limit theorem (CLT).
\item Use empirical quantiles: If phase I has returned enough data, then we may estimate 
$\x_{\alpha/2}$ and $\x_{1-\alpha/2}$ using the empirical quantiles of phase I. 
The false alarm rate will be $\alpha$ since $P(\x \not \in [\hat{\x}_{\alpha/2},\hat{\x}_{1-\alpha/2}]) \approx \alpha$.
This is a \textbf{super practical} way to go about. The only downside of this avenue, is that you do not always have enough in-control data from phase I.
\end{enumerate}




\subsection{Rational Groupings}
\label{sec:rational_grouping}
Recall that at each period we compute the average of $n$ samples. 
To fix ideas, think of a period being a day of production. 
How should we draw samples in this period? 
At the same time from the same machine?
At different times from the same machine?
Many configurations are possible, and the correct approach depends on the type of out-of-control behaviour one seeks. 
\emph{Rational groupings} merely reminds us to sample ``rationally'' in each period. 
Quoting \cite{montgomery_introduction_2007}'s words of caution:
\begin{quotation}
\dots we can often make any process appear to be in statistical control just by stretching out the interval between observations in the sample.
\end{quotation}






\subsection{Other Stopping Rules}
\label{sec:stopping_rules}

The assumption that we may only create alarms if $\bar{x}$ exceeds some control limits is needlessly restrictive.
A first relaxation is by allowing multiple control regions.
It is quite common to define \emph{warning limits} and \emph{action limits}. Each may have its own alarm rate.
We may even change the sapling scheme if limits are breached. Increasing the sampling rate once the warning limits have been breached is known as \emph{adaptive sampling}, or \emph{variable sampling}, and it is a very efficient way to go about. \marginnote{Adaptive Sampling}



Another approach is to define multiple sets of stopping rules.
Here is an example:
\begin{enumerate}
\item One or more points outside of the 3-sigma control limits.
\item Two of three consecutive points outside the 2-sigma limits but still inside the 3-sigma control limits.
\item Four of five consecutive points beyond the 1-sigma limits.
\item A run of 8 consecutive points on one side of the centerline.
\end{enumerate}
The above set of rules is known as the Western Electric Rules, \aka, the \emph{WECO} rules.\marginnote{WECO}
Augmenting the set of rules is the same as increasing a rejection region. It adds more sensitivity, at the cost of false alarms. If the rules are properly selected, the gain in sensitivity is worth the increase in false alarms.

As a quick exercise, we may compute $\alpha$  for $m$ independent rules, each with $\alpha^*$ type I error:
\begin{align}
\label{eq:multiplicity_in_spc}
	\alpha=1-(1-\alpha^*)^m.
\end{align}
Having 4 rules, like WECO, each at $\alpha^*=0.0027$ implies that we actually have $\alpha=0.01$ and $ARL_0 \approx 93$. For daily sampling of an in-control process, this means an alarm every quarter, and not every year. 
The good news is the analysis in Eq.(\ref{eq:multiplicity_in_spc}) does not apply to WECO, because the rules not independent but rather highly dependent. 
A can compute the $ARO_0$ of WECO in a quick simulation.

\begin{extra}[Stopping Rules]
There are many sets of stopping rules. 
These include WECO, Nelson, AIAG, Juran, Hughes, Duncan, Gitlow, Westgard, and more. 
See \url{http://www.quinn-curtis.com/spcnamedrulesets.htm} for a review.
\end{extra}







\section{Shewhart Charts With Other Test Statistics}

We have been focusing on the \barxChart for ease of exposition. There are, however, many cases where the mean is not an appropriate test statistic.
Examples include:
\begin{enumerate}
\item A discrete CTQ, where only the number of non-compliances can be counted. 
\item Where the departure from statistical control is not only a shift in $\mu$. 
\end{enumerate}

The following charts are designed for those cases. 
Practically all of the ideas presented for the \barxChart may be adapted to these other test statistics after appropriate adaptations. 
The reader is referred to \cite{montgomery_introduction_2007} for the details. 

\label{sec:other_control_charts}
\subsection{$R$ Chart}
Where $\bar{x}$ is replaced by the range, $\max_i\set{x_{i,t}}-\min_i\set{x_{i,t}}$.
This chart is sensitive to many changes in the distribution of the CTQ; the variance in particular. 
\subsection{$s$ Chart}
Where $\bar{x}$ is replaced by $s$. 
Sensitive to variability changes. 
This is an important and useful chart which we will revisit in Section~\ref{sec:multivarite_s}.
\subsection{$s^2$ Chart}
Like the $s$ chart, only in variance-scale.
\subsection{Regression Control Chart}
In a \emph{regression control chart}, the test statistic can be a regression coefficient. 
When compounded with multivariate charts, a regression control chart may accommodate several regression  coefficient, or the residuals. 
This is very useful if you allow the distribution of the CTQ to vary with some covariate, and you want to detect a change in this relation. 
To fix ideas, think that your CTQ depends on the temperature at the time of production. The distribution of the CTQ will thus vary with the temperature, but the break in their relation is cause for alarm.
\subsection{$p$ and $np$ Chart}
Where $\bar{x}$ is replaced by the proportion ($p$), or number ($np$), of non-conforming units.
Appropriate for attributes, i.e., categorical CTQs.
\subsection{$c$ Chart}
Like a $np$ chart, but where the number of nonconforming units is replaced with the total number of nonconformances, allowing multiple defects per unit. 
\subsection{$u$ Chart}
Like the $c$ chart, but allowing a variable number of units per period (varying $n$).



\subsection{Derivative Chart}
If the break of control has occurred between periods (``the night shift guys broke it!''), the change in an \barxChart may carry more information (i.e., more power) than the value of $\bar{x}_t$ itself.
We can thus chart, not $\bar{x}_t$ itself, but rather, the \emph{change} in $\bar{x}_t$ between periods. 
It is quite possible $\bar{x}_t$ seems perfectly ok, but that $\bar{x}_t-\bar{x}_{t-1}$ seems highly irregular. 
Generalizing this idea, we can monitoring the \emph{derivative} of some statistic to detect this kinds of changes. 


\section{Pooling Information Over Periods}
\sectionmark{Pooling Periods}
\label{sec:running_windows}

Assume an out-of-control process is a very mild shift of the mean controlled-process ($\mu$).
A power analysis may suggest that this shift is hard to detect, especially if $n$ is not too large (as seen in Figure~\ref{fig:power_function}).  
If the shift persists over periods, we may gain power, i.e., sensitivity, by pooling several periods together. 
We now present several ways to pool information from history. These are typically applied in Phase II, where out-of-control processes are expected to have only mild shifts, and not major ones as in Phase I. 

\begin{remark}[No longer Shewhart]
The name \emph{Shewhart control chart} is reserved to charts plotting one period at a time. 
When several periods are pooled together, we will no longer call this  ``Shewhart''.
\end{remark}

\begin{remark}[One observation at a time]
The following charts have a continuous flavour. As such, it is both favourable, and common, to compute them using one observation at a time, meaning that $n=1$. 
\end{remark}





\subsection{Moving Average Chart (MA)}
\sectionmark{MA}

One way to pool information from different periods is by a \emph{moving average}.
\begin{definition}[MA]
The \emph{moving average} (MA) in a window of $w$ periods ending at period $t$, is defined as
\begin{align}
	M_t:= \frac{x_t+\dots+x_{t-w+1}}{w}.
\end{align}
\end{definition}
Assuming $x_t \sim \gauss{\mu, \sigma_x^2}$ then clearly 
\begin{align}
	M_t \sim \gauss{\mu, \sigma^2_{M_t}=\frac{\sigma_x^2}{w}}.
\end{align}

The control limits on $M_t$ are typically
\begin{align}
	UCL &:= \mu_0 + \arm \sigma_{M_t}= \mu_0 + \arm \, \frac{\sigma_x}{\sqrt{w}}, \\
	LCL &:= \mu_0 - \arm \sigma_{M_t}= \mu_0 - \arm \, \frac{\sigma_x}{\sqrt{w}}.
\end{align}
For $\arm=3$ the false alarm rate of this criterion is trivially $\alpha=0.0027$. 
The $ARL_0$ is no longer simple to compute. 
This is because the pooling of periods has compromised independence between periods, and Eqs.(\ref{eq:arl_0},\ref{eq:arl_1}) are no longer valid. 
Do not despair as the ARL may still be computed. 
You can always use simulation to compute it, or try using the \rcode{spc} \R package.


%\begin{algorithm}[$ARL_0$ simulation for MA Control Chart]
%\caption{$ARL_0$ simulation for MA Control Chart}
%\begin{algorithmic}
%\For {$\bootstrap \in 1,\dots,\bootstraps$}
%	\State $\sample^\bootstrap \gets$ $n$ randomly selected observations, with replacement, from the original data.
%	\State $\sample^\bootstrap_\rank \gets$ $\rank$ randomly selected variables from $\sample^\bootstrap$.
%    \State $\estim{\hyp}^{\bootstrap} \gets$ a tree learned with  $\sample^\bootstrap_\rank$.
%\EndFor
%\State \Return the the average prediction for $x$ over $\estim{\hyp}^{\bootstrap}$ .
%\end{algorithmic}
%\end{algorithm}


We are free to choose the magnitude of $w$. 
If $w$ is too small, there is no real pooling from history. At the limit, where $w=1$, we are back to the classical Shewhart chart. 
If $w$ is too large, then each new observation has very small importance, and it may take a long time to detect a shift.
Which is the right intermediate value of $w$, is left for you to decide, possibly using a power analysis as a function of $w$. 






\subsection{Exponentially Weighted Moving Average Chart (EWMA)}
\sectionmark{EWMA}
The moving average gives all observations the same importance. 
We want to change this, giving more importance to new observations so that we may capture drifts quickly when they occur. 
The \emph{Exponentially Weighted Moving Average} (EWMA), \aka the \emph{geometric moving average} (GMA), does just that. \marginnote{GMA}
\begin{definition}[EWMA]
For a fixed $\lambda \in [0,1]$, the \emph{exponentially weighted moving average} (EWMA) is defined as 
\begin{align}
	z_t &:= \lambda x_t + (1-\lambda) z_{t-1}
\end{align}
\end{definition}
By recursive substitution, we have 
\begin{align}
	z_t &= \lambda \sum_{j=0}^{t-1} (1-\lambda)^j x_{t-j} + (1-\lambda)^t z_0.
\end{align}
Assuming $x_t \sim \gauss{\mu, \sigma_x^2}$ then  
\begin{align}
\label{eq:ewma_variance}
	z_t &\sim \gauss{\mu_0,	\sigma^2_{z_t} }, \\
	\sigma^2_{z_t} &= \sigma^2_x \left( \frac{\lambda}{2-\lambda} \right)(1-(1-\lambda)^{2t}).
\end{align}
Eq.(\ref{eq:ewma_variance}) may be used to construct control limits for EWMA.
It is however, more economic to observe that for large $t$ then  $(1-(1-\lambda)^{2t}) \approx 1$ so that we may use 
\begin{align}
\begin{split}
\label{eq:ewma_variance_approximate}
	UCL &:= \mu_0 + \arm \sigma_{z_t} \approx \mu_0 + \arm \, \sqrt{\sigma^2_x\left( \frac{\lambda}{2-\lambda} \right)},  \\
	LCL &:= \mu_0 - \arm \sigma_{z_t} \approx \mu_0 - \arm \, \sqrt{\sigma^2_x\left( \frac{\lambda}{2-\lambda} \right)},
\end{split}
\end{align}
with $\arm=3$ being the typical choice.
By now, you should immediately know what is the false alarm rate of these limits.
By now, you should also know that because of the dependence between $z_t$'s, computing the ARL is not as simple as for Shewhart charts. The \rcode{xewma.arl()} \R function, in package \rcode{spc}, permits doing so easily. 
Its output for various $\lambda$ and $\arm$ is illustrated in Figure~\ref{fig:arl_0_ewma}.

\begin{figure}[h]
\centering
\includegraphics[height=0.3\textheight]{art/fig53}
\caption[$ARL_0$ for EWMA]{$ARL_0$ for EWMA. \newline Code from \url{http://users.phhp.ufl.edu/pqiu/research/book/spc/r-codes/fig53.r}}
\label{fig:arl_0_ewma}
\end{figure}

In the MA chart, we used the choice of $w$ to balance between quick response (small $w$) and sensitivity (large $w$).
EWMA has no window-width parameter, since it looks into all of history. On the other hand, we can control it by choosing $\lambda$. 
Large $\lambda$ gives more importance to the present. At the limit, $\lambda=1$, EWMA collapses to a standard Shewhart chart.







\subsection[CUSUM]{CUSUM Chart}
The \emph{cumulative sum} chart is similar to the EWMA in that it pools information from the history. 
The CUSUM simply sums all past deviations from the centre line.
If the process is in control, deviation will cancel each other, and their sum will vary around $0$. 
If the process is out of control, a drift will appear. 
The statistic to be plotted is 
\begin{align}
	C_t:= \sum_{j=0}^{t}(x_j-\mu_0) = C_{t-1}+ (x_t-\mu_0).
\end{align} 
Assuming $x_t \sim \gauss{\mu, \sigma_x^2}$ then if under control then $C_t \sim \gauss{\mu_0, t \sigma_x^2}$, we could thus set 
\begin{align}
\label{eq:cusum_simple_limits}
\begin{split}
	UCL &:= \mu_0 + \arm \sigma_{C_t}= \mu_0 + \arm \, \sqrt{t \sigma_x^2},  \\
	LCL &:= \mu_0 - \arm \sigma_{C_t}= \mu_0 - \arm \, \sqrt{t \sigma_x^2},
\end{split}
\end{align}
and $\arm=3$ as usual. 
You may encounter these limits in your favourite software (\rcode{qcc} package in \R), but it less often discussed in the literature. \cite{montgomery_introduction_2007} for example discuses very different limits. The discrepancy is explained in the following Extra Info. 

\begin{extra}
CUSUMs were introduced by \cite{page_continuous_1954}. 
\cite{montgomery_introduction_2007} adopts \citeauthor{page_continuous_1954}'s view and presents limits in two forms: the \emph{decision interval} (DI) form, \aka the \emph{tabular} form, and the graphical form known as a \emph{V-mask}.\marginnote{V-Mask}
These two control limits are equivalent. 
The fundamental difference between the control limits of \cite{page_continuous_1954}, and the ones presented until now, is that \citeauthor{page_continuous_1954} designed limits for the particular history of each process, while the limits until now, including Eq.(\ref{eq:cusum_simple_limits}) do not adapt to the particular history of the process.
As such, \citeauthor{page_continuous_1954}'s control limits are said to be \emph{adaptive}.\marginnote{Adaptive Contol Limits}
The limits in \cite{page_continuous_1954} are far from intuitive. 
This author has found \cite{ritov_decision_1990} to be the best explanation of \cite{page_continuous_1954}, but do not expect an easy reading...
\end{extra}









\section[Multivariate]{Multivariate Control Charts for Location}
\label{sec:multivariate}
% Wishart
% Srivastava Du
% PCA
% Higher criticism

\begin{example}[Intensive Care Unit]
\label{eg:intensive}
Consider an intensive care unit. 
The CTQs are the patient's blood pressure, temperature, etc.
We want to sound an alarm if the patient's condition deteriorates. 
Clearly, we can apply the univariate methodology above on each CTQ.
It is possible, that the deterioration is mild, so that it is not picked up by any CTQ individually (low power), but could have been noticed were we to aggregate signal over various CTQs. 
This is the concern of the current section. 
\end{example}


\subsection{Mass Univariate Control}
\label{sec:mass_univariate}

A first natural approach is to raise an alarm when \textbf{any} of the processes exceeds its respective control limits.
For $p$ independent processes, with univariate false alarm rate $\alpha^*$ each, then the joint false alarm rate is 
\begin{align}
	\alpha = 1-(1-\alpha^*)^p.
\end{align}
Clearly we could set $\alpha^*=1-\sqrt[p]{1-\alpha}$, so that the joint false alarm rate is under control, but we would not be enjoying the added sensitivity of pooling many CTQs together. 



\subsection{Hotteling's \tsq}
Hotteling's \tsq statistic is a generalization of the t-test.
To see this we write the t-statistic in the following weird form:
\begin{align}
	t^2(x_t)=(\bar{x}_t-\mu_0) (s^2_t(x))^{-1} (\bar{x}_t-\mu_0).
\end{align}
This notation readily extends to the multivariate case. 
For $p$ CTQs, then $\bar{x}_t$ and $\mu_0$ are $p$-length vectors, and $s^2_t(x)$ is replaced with the $p \times p$ covariance matrix $\hat{\Sigma}(x)$.
Both typically estimated from Phase I. 
\begin{definition}[Hotteling's \tsq]
\begin{align}
\label{eq:hotteling}
	T^2(x_t) := n (\bar{x}_t-\hat{\mu}_0)' \hat{\Sigma}^{-1} (\bar{x}_t-\hat{\mu}_0).
\end{align}
\end{definition}
To derive the control limits, we will be assuming that $\x_{it}$ is $p$-variate Gaussian distributed, $\x_{it}\sim \gauss{\mu_0, \Sigma_{p \times p}}$. 
Put differently, we assume that the $i$'th sample at period $t$ is a $p$-vector, such that its $j$'th coordinate $\x_{ijt}$ is univariate Gaussian, with mean $\mu_{0,j}$ variance $\Sigma_{j,j}$, and covariance with some other CTQ $\x_{ij't}$ given by $\cov{\x_{ijt},\x_{ij't}}=\Sigma_{j,j'}$. 

The sampling distribution of $T^2_t$ may depend on how $\mu_0$ and $\Sigma_0$ are estimated. 
For our purposes, where $\mu_0$ and $\Sigma_0$ are estimated in Phase I, we can safely use the following approximation
\begin{align}
	T^2_t \overset{H_0}{\rightsquigarrow }\chi^2_p.
\end{align}
We can thus construct the control limit for this scenario:
\begin{align}
	UCL:= \chi^2_{1-\alpha,p}.
\end{align}
Note that the \tsq statistic is \emph{non directional}: it will increase in the presence of both positive and negative drift, so that there is no LCL.

Since the above limits have an (approximate) type-I error rate of $\alpha$, and the periods are typically independent, then we can readily apply Eq.(\ref{eq:arl_0}) to compute $ARL_0$.

\begin{remark}
We may consider the exact distribution of \tsq under many configurations. 
We opted for the $\chi^2_p$ for ease of exposition. 
For exact result \cite[Ch.7]{qiu_introduction_2013} should be consulted. 
\end{remark}






\section[Multivariate extensions]{Multivariate Control- Extensions}

Hotelling's \tsq is perhaps the first and most natural multivariate test statistic.
There are, however, circumstances where we can choose a better statistic.


\begin{example}[Cyber Monitoring System]
\label{eg:cyber}
Consider a server farm. 
All servers dump their status into logs. These include CPU loads, temperature, network I/O, etc.
The administrator is worried about an imminent attack, and is thus parsing the logs for CTQs, and inspecting the on-line status on his dashboard.
He knows that the cyber-attacker is no amateur, so that if any fingerprint is left in the logs, it will be very subtle and manifested in very few CTQs\footnote{Clearly, these are not called CTQs, in this context. We are simply adhering to quality engineering terminology.}.
\end{example}


\begin{example}[Vine Health]
\label{eg:irrigation}
Consider vine crops.
The crops are monitored for their health.
The health measurements (CTQs) may be considerably affected by irrigation, weather, etc.
An unhealthy vine is thus one that behaves differently than the rest. 
The signal for out-of-control we seek is thus not in the mean CTQ of the vine, but rather in its correlation with others. 
We would like a multivariate version of the $s$ chart to monitor the health of our vines.  
\end{example}


Based on these examples, we list cases where \tsq is a \textbf{bad} choice. Appropriate test statistics for these cases are discussed in the following sections.
\begin{description}

\item [Multivariate $s$ chart] The signal we seek is not in the mean of the CTQ but rather in its variance, such as Example~\ref{eg:irrigation}. Put differently, we actually want a multivariate version of the $s$ chart and not the \barxChart. 

\item [Sprase signal] The signal we seek is expected to exist in a small subset of the $p$ coordinates, like in the cyber-security of Example~\ref{eg:cyber}. This is known as a \emph{sparse signal}, unlike the \emph{dense signal} targeted by the \tsq. \marginnote{Sparse Signal}

\item [High dimension] \tsq is simply incomputable. From Eq.(\ref{eq:hotteling}) we see that computing \tsq required the inversion of the covariance matrix. Often this is an impossible task. We call this a \emph{high-dimensional problem}.
\end{description}




\subsection{Mutivariate $s$ chart}
\label{sec:multivarite_s}

As discussed in the vine health example (Example \ref{eg:irrigation}), there are cases where the out-of-control behaviour is manifested in the covariance between CTQs, and not in their mean.
Recalling the Exploratory Data Analysis section (\ref{sec:exploratory}), the most popular measure of multivariate relation is the covariance matrix (Definition~\ref{def:covariance}).
While we already know how to plot a matrix, plotting the evolution of a matrix in time, as is required for a control chart, is simple task. 
Without going into the details of multivariate statistics, the fundamental idea is to summarize the matrix by a single number, so that it may be easily plotted. 
To do so, we recall that in vector spaces, \emph{norm functions} measure distances from the origin.
This idea may be generalized to matrix spaces. By using \emph{matrix norms} each matrix is summarized into a single number which can be interpreted as its distance from the origin (a matrix of zeroes).
The most popular matrix norms include:

\begin{definition}[Operator norm]
Operator norm views the matrix as a the linear function it represents, so that the distance of a matrix from the origin is captured by the displacement this linear function causes upon its (normalized) inputs.
\begin{align}
	\norm{A}_{Op}:= \sup_x \set{\norm{Ax}; \norm{x}=1}
\end{align}
\end{definition}


\begin{definition}[Frobenius (matrix) norm]
The \emph{Frobenius norm}, \aka the \emph{Hilbert-Schmidt norm}, views the matrix as a square of numbers and is defined as follows:
\begin{align}
	\normF{A}:= \sqrt{\sum_{i,j} a_{i,j}^2} = \sqrt{\Tr(A'A)}
\end{align}
\end{definition}

\begin{extra}[Frobenius matrix norm]
Another definition of the Frobenius matrix norm is via its singular values:
$ 	\normF{A}:= \sqrt{\sum \sigma_i}, $
where $\sigma_i$ is the singular values of matrix $A$.
\end{extra}


\begin{extra}[Matrices and graphs]
Since a matrix defines a \emph{weighted graph} (and vise-versa), \aka a \emph{network}, any network similarity measure can be used to measure the distances between matrices, and can thus be used to summarize the covariance matrix and plot in a control chart.
\end{extra}





\subsection{Sparse Signals}

We have already claimed that if signal exists only in a small subset of coordinates, i.e., it is sparse, we can do better than \tsq, at least in the sense of power.
The mass-univariate test is an example which is preferred over \tsq is the signal is sparse.



\begin{extra}[Sparse Signal Detectors]

The fundamental idea for constructing sparse signal detectors is to compute some test statistic for each of the $p$ coordinates, and then test for the existence of signal in any of them. 
Denoting $x_{i,j,t}$ be the $i$'th measurement of attribute $j$ in period $t$.
Now omitting the period index $t$ for brevity, we denote $z_j=z_j(x_{1,j},\dots,x_{n,j})$ be the z-score of the $j$ coordinate at period $t$. 
If there is no signal in any coordinate then all the $z_j$ should be distributed $\gauss{0,1}$ (that is their null distribution, right?)
If a signal exists, then the $z_j$ will not look like a sample from the $\gauss{0,1}$ distribution. 
We now see that detecting a sparse signal, is ultimately the same as the goodness of fit problems of Section~\ref{sec:gof}, where we test if $(z_1,\dots,z_p)$ looks like a $p$-sized sample from the $\gauss{0,1}$ distribution.

\end{extra}





\subsection{High-Dimensional Control Charts}

By \emph{high dimension} we don't mean that $p$ is very large but rather the it is larger then $n$.
This means that in each period we many observations, but we have ever more coordinates in the CTQ.
To fix ideas, imagine that in the intensive care unit, in each of $n=10$ measurements per day, we take $p=100$ different measurements such as pulse, temperature, etc.


The problem in high-dimension is that we cannot properly estimate the covariance. 
For reasons we will not dive into, the most common manifestation is that the covariance matrix $\hat{\Sigma}$ in Eq.(\ref{eq:hotteling}) is not invertible so that \tsq is not only inefficient, but cannot even be computed!
The fundamental idea to deal with the high-dimension is, well, to ignore the covariance.
Two matters need to be considered when doing so:
(i) Is the type I error rate conserved?
(ii) Is power lost?
The answer to the first is- yes, but some corrections are required.
The answer to the second is- yes, but hopefully not too much.

\begin{think}
How many free parameters are in the covariance between $p$ measurements?
How many observations we would thus need to estimate this covariance?
\end{think} 



An example of a ``covariance agnostic'' signal detector, adapted from \cite{srivastava_test_2008}, is the following \footnote{I have simplified the actual statistic, so the reader is enouraged to read the reference before using the statistic.}:
\begin{definition}[Srivastava-Du statistic]
\begin{align}
\label{eq:srivastava_du}
	 	T^{SD}(x) := n (\bar{x}-\hat{\mu}_0)' D^{-1} (\bar{x}-\hat{\mu}_0),
\end{align}
where $D$ is $\hat{\Sigma}$ with zeroes in all off-diagonal entries. Put differently, $T^{SD}$ is the same as \tsq in Eq.(\ref{eq:hotteling}) under an independence assumption.
\end{definition}
Changing from matrix notation to sum notation, we see that 
\begin{align}
	 	T^{SD}(x) := n \sum_{j=1}^p (\bar{x}_j - \hat{\mu}_{0,j})^2 D^{-1}_{j,j} .
\end{align}
But wait! 
$D^{-1}_{j,j}$ is just $\frac{1}{Var(x_{1,j},\dots,x_{n,j})}$!
Why is this exciting? 
Because it means that Eq.(\ref{eq:srivastava_du}) is nothing but the sum of the squared univariate t-statistics over the $p$ coordinates:
\begin{align}
	 	T^{SD}(x) :=  \sum_{j=1}^p \frac{(\bar{x}_j - \hat{\mu}_{0,j})^2}{\var{\bar{x}_j}}  .
\end{align}
We now see that a way to deal with the high-dimension, is to perform $p$ marginal tests, and then aggregate them.
Denoting the t-statistic of the $j$'th coordinate by $t_j:= \frac{(\bar{x}_j - \hat{\mu}_{0,j})^2}{\var{\bar{x}_j}}$ we can now easily design many high dimensional signal detectors in the form of
\begin{align}
	 	T(x) :=  f(t_1,\dots,t_p) .
\end{align}
where $f$ is some distance function\footnote{A norm function to be more precise.}.
Clearly, for $T^{SD}$ we need to set $f(t_1,\dots,t_p) = \normII{(t_1,\dots,t_p)}^2$.
It may be less obvious, but for the mass-univariate test in Section~\ref{sec:mass_univariate} we need to use $f(t_1,\dots,t_p)=\max\set{t_1,\dots,t_p}$.



\subsection{Temporal Pooling of Multivariate Charts}
The basic problem: can we gain power by pooling some multivariate statistic over several periods?
Sure!
The most basic way to do so:
\begin{definition}[Moving sum Hotelling]
The \emph{moving sum Hotelling} chart in a window of $w$ periods ending at $t$, is defined as
\begin{align}
	M_t:= T^2(X_t)+\dots+T^2(X_{t-w+1})
\end{align}
where \tsq is Hotelling's test statistic $X_t$ is the matrix of $n$ samples of $p$ measurements at period $t$. 
\end{definition}

Since $T^2(X_t)$ is approximately $\chi^2_p$ distributed, periods are independent, and sums of Chi-square distributions are still Chi-square, with the summed degrees of freedom, we have that 
\begin{align}
	M_t \sim \chi^2_{wp}.
\end{align}
This readily suggests that the UCL for this chart is $\chi^2_{1-\alpha,wp}$. 

\begin{think}
Is the moving sum Hotelling a Shewhart chart? 
Can you design more multivariate charts with period pooling? 
Can you derive their control limits?
\end{think}





\section{The p-value chart}
The basic construction of the previous control charts was to choose some test statistic that is sensitive to the signal  we seek (read-alternative hypothesis). We then derive its sampling distribution, and plot it against some control limits. 
From the hypothesis testing literature we know that we can convert the test statistic to a p-value and make the same inference. 
This idea can carry to control charts.

Figure~\ref{fig:p-value-chart} demonstrates a control chart based on the p-value at each period.
These charts have the advantage that once the p-values have been computed, the control limits are fixed (for example at $0.05$).
They are always in the same scale, regardless of the underlying process and test statistic. 
In this sense, the p-value charts unify all other control charts (with some exceptions, admittedly). 
The caveat, as in statistical hypothesis testing, is that computing the p-value may be a formidable task. 

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{art/p-value-chart}
\caption{A p-value control chart. Source: \cite{li_using_2012}.}
\label{fig:p-value-chart}
\end{figure}





\section{Economical Design of Control Charts}
\label{sec:economical_considerations}

[TODO: rewrite]


Up until now, our design of control charts was driven by type-I error rates, and ARLs. Economical consideration were merely implied.
In this section, economical consideration take the driver's seat. 
We present a toy model, to demonstrate the economical optimization of design parameters in a \barxChart. 
Before beginning, a few remarks are in order. 

\begin{remark}[Economical Design of Control Charts]
\noindent
\begin{enumerate}
\item According to \cite{montgomery_introduction_2007} 
\begin{quote}
Saniga and Shirland (1977) and Chiu and Wetherill (1975) report that \textbf{very few practitioners} have implemented economic models for the design of control charts.
\end{quote}
Hmmmm.. Have things changed since 1977?
\item A comprehensive theoretical analysis of the optimization of a quality control system may be found in \cite{girshick_bayes_1952}. Again, \cite{montgomery_introduction_2007} is skeptic:
\begin{quote}
The optimal control rules are difficult to derive, as they depend on the solution to complex integral equations. Consequently, \textbf{the modelâ€™s use in practice has been very limited}.
\end{quote}
\end{enumerate}
\end{remark}

In light of the above skepticism, and following the lines of \cite{duncan_economic_1956}, we aim at the modest goal of an economical optimal \barxChart. 
Our target function is optimizing the expected income per hour, with respect to the design parameters:
\begin{align}
\label{eq:optimal_design}
	\max_{n,\arm,h}\set{\expect{C/T}}
\end{align}
where $C$ is the income between two productions halts, i.e., a \emph{cycle};
$T$ is the cycle duration;
$n$ is the number of samples per period;
$\arm$ governs the control limits via $UCL:= \mu_0+ \arm \sigma_{\bar{x}}= \mu_0+ \arm \sigma_x / \sqrt{n}$;
$h$ is the hours between sample periods. 

We now need to establish how $\expect{C/T}$ is related to $n,\arm,h$. Here is our set of assumptions and notation:
\begin{enumerate}
\item When in control (IC), production is centred on $\mu_0$, assumed known. 
\item When out of control (OC), $\mu_1=\mu_0 \pm k \sigma_x$. 
\item When OC, production may proceed (!). 
\item Search and repair costs are not part of $C$.
\item OCs occur as a Poisson process, with rate $\lambda$ events per hour. The expected time from a sampling to an OC events is thus 
\begin{align}
	\tau := \frac{1-(1+ \lambda h) e^{-\lambda h}}{\lambda(1-e^{-\lambda h})}.
\end{align} 
\item The power to detect an OC is 
\begin{align}
	\pi:= \Phi(-\arm-k/\sqrt{n})+ (1-\Phi(\arm-k/\sqrt{n})).
\end{align}
\item The false alarm rate
\begin{align}
	\alpha:= 2 \Phi(-\arm).
\end{align}
\item Because of the Poisson process assumption,  $\expect{C/T}=\expect{C}/\expect{T}$. 
\item The expected cycle length:
\begin{align}
	\expect{T}= \frac{1}{\lambda}+ \frac{h}{\pi}- \tau  + D.
\end{align}
where $\frac{1}{\lambda}$ is time IC;
$\frac{h}{\pi}- \tau$ is the time the process is OC until detection;
$D$ is a fixed time to identify the assignable cause. 
\item The expected income per cycle
\begin{align*}
	\expect{C}= V_0 \frac{1}{\lambda} + 
	V_1 \left(\frac{h}{\pi}- \tau  + D  \right) - 
	a_3 -
	\frac{a'_3 e^{-\lambda h}}{1-e^{-\lambda h}} -
	(a_1+a_2n)\frac{\expect{T}}{h}
\end{align*}
where $V_0$ is the net income per cycle when IC;
$V_1$ is the net income when OC;
$(a_1+a_2n)$ is the fixed and variable cost of taking a sample;
$a_3$ is the cost of finding an assignable cause;
$a'_3$ is the cost of investigating a false alarm.
\end{enumerate}

[TODO: add timeline]

Given all the above, we may now plug Eq.(\ref{eq:optimal_design}) into our favourite numerical solver to find the optimal $h,\arm,n$.





\section{Bibliographic Notes}
The contents of this chapter is mostly derived from \cite{montgomery_introduction_2007}. 
For a more mathematically rigorous treatment of the topic see \cite{basseville_detection_1993}.
For an \R oriented exposition of the topic, see \cite{qiu_introduction_2013}.
A quick digest review may be found in \cite{natrella_nist/sematech_2010}.
For multivariate process control, see for example \cite{ge_multivariate_2012}. 
For a technical discussion of multivariate statistics see \cite{anderson_introduction_2003}. 
For some recent advances on high-dimension multivariate tests see \cite{srivastava_testing_2013} and references therein.
